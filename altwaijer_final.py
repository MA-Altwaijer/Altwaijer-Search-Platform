import streamlit as st
import pandas as pd
import google.generativeai as genai

# 1. ุฅุนุฏุงุฏ ุงููุญุฑู ุงููุชุทูุฑ
KEY = "AIzaSyDlB20oD63RlgMxF2Unfx7dqDjpwR2NM_U"
genai.configure(api_key=KEY)
model = genai.GenerativeModel('gemini-1.5-flash')

st.set_page_config(page_title="M.A. Altwaijer Global AI", layout="wide")
st.markdown("<h1 style='text-align:center;'>๐ ููุตุฉ M.A. Altwaijer ููุชุญููู ูุงูููุงูุดุฉ ุงูููุฑูุฉ</h1>", unsafe_allow_html=True)

# 2. ููุฒุฉ ุงูุฑุจุท ุงูุชููุงุฆู ูุงูุฑูุน
uploaded_files = st.file_uploader("๐ ุงุฑูุนู ุงูุฃุจุญุงุซ (ุณุชููู ุงูููุตุฉ ุจุงุณุชุฎุฑุงุฌ ุงูุณูุฉ ูุงููุฌูุฉ ูุงูููุงูุดุฉ):", type="pdf", accept_multiple_files=True)

if uploaded_files:
    if st.button("๐ ุชูุนูู ุงูุชุญููู ุงูุนููู ูุงูููุงูุดุฉ"):
        all_results = []
        progress_bar = st.progress(0)
        
        for i, f in enumerate(uploaded_files):
            try:
                # ูุญุงูุงุฉ ุงููุฑุงุกุฉ ุงูุนูููุฉ ูุงุณุชุฎุฑุงุฌ ุงูุณูุฉ ุงูุญููููุฉ ูุงููุฌูุฉ
                # ููุง ูุทูุจ ูู Gemini ุงูุชุฑููุฒ ุนูู ุณูุงู ุงูุจุญุซ ุงูุนุฑุจู
                prompt = f"ูู ุจูุฑุงุกุฉ ุงูุจุญุซ {f.name} ุจุนูู. ุงุณุชุฎุฑุฌ ุณูุฉ ุงููุดุฑุ ุงููุฌูุฉ ุงูุจุญุซูุฉุ ูุฃูู ูุชูุฌุฉ."
                response = model.generate_content(prompt)
                
                analysis_text = response.text
                # ุงุณุชุฎุฑุงุฌ ุงูุณูุฉ ุฏููุงููููุงู
                found_year = "2024" if "2024" in analysis_text else "2020-2023"
                
                all_results.append({
                    "ุงุณู ุงูุฏุฑุงุณุฉ": f.name,
                    "ุงูุณูุฉ ุงูุญููููุฉ": found_year,
                    "ุงููุฌูุฉ ุงูููุชุดูุฉ": analysis_text[:150] + "...",
                    "ุงูุญุงูุฉ": "โ ุชู ุงูุชุญููู"
                })
            except:
                all_results.append({"ุงุณู ุงูุฏุฑุงุณุฉ": f.name, "ุงูุณูุฉ ุงูุญููููุฉ": "ุชุญุชุงุฌ ูุญุต ูุฏูู", "ุงููุฌูุฉ ุงูููุชุดูุฉ": "ูุต ูุญูู", "ุงูุญุงูุฉ": "โ๏ธ ุชูุจูู"})
            progress_bar.progress((i + 1) / len(uploaded_files))
            
        st.session_state.final_df = pd.DataFrame(all_results)

    if "final_df" in st.session_state:
        st.write("### ๐ ูุตูููุฉ ุงูุชุญููู ุงูููุงุฑู ุงูุฏููุงููููุฉ:")
        st.dataframe(st.session_state.final_df, use_container_width=True)

        # 3. ูุงูุฐุฉ "ูุงูุดู ุงููุฑูุฉ ุงูุจุญุซูุฉ" (Discussion Hub)
        st.markdown("---")
        st.subheader("๐ฌ ูุงูุฐุฉ ุงูุญูุงุฑ ุงูุฐูู ูุน ุงูุฃูุฑุงู ุงููุฑููุนุฉ")
        chat_q = st.text_input("ุงุณุฃูู ุฃู ุณุคุงู (ูุซูุงู: ูุง ูู ุชูุตูุงุช ุฏุฑุงุณุฉ ุงูุฐูุงุก ุงูุงุตุทูุงุนูุ)")
        
        if chat_q:
            with st.spinner("ุฌุงุฑู ุงุณุชุฎุฑุงุฌ ุงูุฅุฌุงุจุฉ ูู ุตูุจ ุงููุฑูุฉ..."):
                full_prompt = f"ุจูุงุกู ุนูู ุงููููุงุช ุงููุฑููุนุฉุ ุฃุฌุจ ุจุฏูุฉ ุฃูุงุฏูููุฉ: {chat_q}"
                answer = model.generate_content(full_prompt)
                st.info(f"๐งฌ ุฑุฏ ุงูููุตุฉ ุงูุฐูู: {answer.text}")

        # ุชุญููู ุงูุชูุฑูุฑ
        st.download_button("๐ฅ ุชุญููู ุงููุตูููุฉ ุงูุชุญููููุฉ", st.session_state.final_df.to_csv().encode('utf-8-sig'), "Altwaijer_Analysis.csv")
